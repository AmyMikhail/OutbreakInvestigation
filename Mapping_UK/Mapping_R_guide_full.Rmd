---
title: "Outbreak Investigation module R guide" 
subtitle: "Mapping in R: STEC raw milk tutorial"
author: "Daniel Gardiner and Amy Mikhail"
date: "`r format(Sys.Date(), format = '<br/>Updated on %d %B %Y')`"
output:
  html_document:
    code_folding: hide
    toc: true
    toc_float:
      toc_collapsed: false
    toc_depth: 3
  pdf_document:
    toc: true
    toc_depth: 3
  word_document:
    toc: true
    toc_depth: 3
theme: sandstone
geometry: margin = 1.5cm
editor_options: 
  markdown: 
    wrap: 72
urlcolor: blue
always_allow_html: true
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, 
                      eval = TRUE,
                      message = FALSE, 
                      warning = FALSE, 
                      ft.align = "left",
                      #fig.width = 12,
                      out.width = "100%")
```


```{r restrict output, echo=FALSE}

################################################################
# FUNCTION TO RESTRICT RESULTS TO A FEW LINES OF OUTPUT:

# Check if knitr is installed, if not install it:
if (!requireNamespace("knitr", quietly = TRUE)) install.packages("knitr")

# Load knitr
library(knitr)

# Define empty output:
hook_output <- knit_hooks$get("output")

# Function to restrict lines of output:
knit_hooks$set(output = function(x, options) {
  lines <- options$output.lines
  if (is.null(lines)) {
    return(hook_output(x, options))  # pass to default hook
  }
  x <- unlist(strsplit(x, "\n"))
  more <- "..."
  if (length(lines)==1) {        # first n lines
    if (length(x) > lines) {
      # truncate the output, but add ....
      x <- c(head(x, lines), more)
    }
  } else {
    x <- c(more, x[lines], more)
  }
  # paste these lines together
  x <- paste(c(x, ""), collapse = "\n")
  hook_output(x, options)
})

``` 


\newpage

------------------------------------------------------------------------

# Introduction {.tabset .tabset-pills}

## Version history

This case study was originally developed by Amy Mikhail and Daniel Gardiner for a beginner's R course for epidemiologists that was run in Public Health England from 2017 to 2019.  The case study is based on an outbreak that occurred in the South West of England in 2014.  The data sets used in this exercise are derived from the original outbreak, but they have been anonymised (postcodes jittered and patient identifying information removed).  

The material was then adapted slightly for the UK FETP Outbreak Tools module in 2019.

Substantial changes were made prior to the EPIET Outbreak Investigation module in Berlin in 2021.  In particular:

  + All packages were updated and replaced with tidyverse compatible packages
  + The example code has been substantially updated and converted to tidyverse syntax.
  + The code has been simplified to minimise the number of data transformation steps.

Further modifications were made based on user feedback after the module and prior to running it during the EAN GIS mini-module in Rome in 2022. 

The latest version in preparation for the EPIET Outbreak Investigation module in Berlin in 2022 includes minor modifications to the formatting to make the HTML document easier to navigate, and the creation of a Github io page as a new online home for this tutorial.

\newpage

## Guide for use

This tutorial has been developed to demonstrate how to create static and interactive point and choropleth maps in R.  The tutorial follows a case study based on a real outbreak that occurred in the South West of England and required some geo-spatial analysis to solve. Note that all the demonstrated steps can be applied to data from any country.   

It is structured as follows:

   + A participant pack is provided containing this guide and the required datasets
   + Some background text introduces the case study and provides questions to answer
   + Each section is introduced with some further background
   + The R code needed to create each map is explained in the text
   + To view or hide the R code, toggle the `Show / hide code` button at the top of this document, or individually above each code chunk.
   + The maps are then displayed.
   + Appendix A is an optional exercise demonstrating how to clean postal (zip) codes that conform to a standard format with a regular expression.
   + Appendix B is an optional exercise demonstrating how to map addresses to GPS coordinates.
   + Appendix C provides further details on how the density mapping parameters are set.

Note that this tutorial uses `tidyverse` packages and syntax.  It follows the strategies demonstrated in the [GIS basics chapter of the Epidemiologist R handbook](https://epirhandbook.com/en/gis-basics.html).

Map tiles in this tutorial are now downloaded from OpenStreetMap, as this service is free. We no longer recommend using map tiles from Google maps as access to their service is now by paid prescription only.

For each processing step, **one example** has been provided; in order to answer the case study questions, you may need to repeat this code for other variables in the data set or adapt some of the arguments as needed.  Each exercise includes an `Exploratory

Finally, note that wherever a package that is not part of base R has been used, the function is preceded by the package name, e.g.: `stringr::str_replace()` is the `str_replace` function from the `stringr` package.  This should make it easier to identify which packages you will need to use when writing your own mapping code.  For the same reason, wherever possible arguments within functions have been explicitly named, so that you can identify and read about the accepted values for these arguments in the package help files.

\newpage

## Prerequisites

This tutorial is not intended to be an introduction to using R; if you are new to R, we recommend that you work through the first 16 chapters in the `Basics` and `Data management` sections of the [Epidemiologist R handbook](https://epirhandbook.com/en/index.html), prior to taking this course.  This tutorial follows the same processes and packages to import and clean the data prior to mapping.

If you are still struggling to understand the R code in this tutorial, don't worry; we recommend that you focus on understanding how each type of map is used to aid the investigation and how to interpret the output.  If you wish to follow this tutorial without executing the code, you can hide the code chunks by toggling the `Show / hide code` button at the top right of this document.

If you do feel comfortable enough to try out the code, we suggest that you work with the un-annotated R markdown document provided in the participant pack.  It contains the same code chunks as this document, but without the explanatory text.  A good way to become familiar with the code is by modifying the arguments and then running the chunk again, to see what effects this has on the output.  

If you have succesfully completed this tutorial and have some extra time, you may wish to try adapting the code and trying it out on your own geo-spatial data.    

\newpage

## R Setup

### Software requirements

To work through this tutorial, you will need:

   + recent version of R (>= 4.2.x) 
   + recent version of Rstudio (>= 2022.07.x)
   + required packages installed (see below)
   + this tutorial (available in the participant pack and online [here]())
   + datasets (available in the participant pack [here]())
   + internet connection (required for geocoding and downloading static maps)


### Installing R packages

This section describes how to install and load the packages that you will need for this mapping tutorial.  Some brief details on why each package was selected are provided below:

   + `pacman` - Checks for, installs (if needed) and loads multiple packages at once
   + `rio` - import multiple different data types with a single command
   + `here` - import and export data using relative file paths
   + `janitor` - handy functions for data cleaning
   + `tidyverse` - collection of packages based on tidy data format and dplyr syntax
   + `tidygeocoder` - geocoding (converting UK postcodes to longitude and latitude)
   + `sf` - converts meta data and coordinates to a "simple feature" object for maps 
   + `osmdata` - free OpenStreetMap resource for creating map boundary boxes
   + `ggmap` - "grammar of graphics" package for creating and displaying static maps
   + `scales` - create pretty breaks for choropleth maps
   + `leaflet` - java package for making interactive maps with more complex features
   + `htmlwidgets` - allows saving of leaflet maps as html files
   
To ensure that the package installation goes smoothly, you have been provided with a separate package installation script.  

  1. Prior to running this script, you should close any open instances of R and RStudio
  2. Open a fresh instance of RStudio
  3. Within RStudio, browse for and open the package installation script
  4. Restart R by going to `Session` and then `Restart R` in the top toolbar of the RStudio menu as shown in the image below.
  5. Run the code in the script as instructed, to install the packages.
  6. If prompted to update or install additional packages, select the `From CRAN` option.
  7. If you encounter any issues, please contact the organizers for help.


![Restarting R from RStudio](images/Restart_r.png)


After starting a fresh RStudio and R session, you will first need to install the `pacman` package.  This package has a function which makes installing and loading other packages much easier.  The code below will first check to see if `pacman`is already installed in your R library, and if not, install it:  

```{r package_installs}

##################################################################
# INSTALL PACKAGES

# Check if the 'pacman' package is installed, if not install it:
if (!requireNamespace("pacman", quietly = TRUE)) install.packages("pacman")


```

Next, you can use the `pacman::p_load()` function to check if the rest of the packages required for this tutorial are already installed in your R library.  Any missing packages will be automatically installed with this function.  

```{r package_loading}

##################################################################
# LOAD LIBRARIES

# Load the required libraries into the current R session:
pacman::p_load(rio,
               here,
               janitor,
               tidyverse,
               tidygeocoder,
               sf,
               osmdata,
               ggmap,
               scales,
               leaflet,
               htmlwidgets)

```

Note that each time you restart R or RStudio, you will need to rerun the `pacman::p_load()` function to load the required libraries into your current session.  We therefore suggest you copy this code and paste it at the top of the R script that you will use for this exercise. 

Adding the argument `update = TRUE` will automatically install the most recent versions of the listed packages if those versions are higher than the ones in your R library.  Once you have installed the packages for this course, we suggest setting this option to `FALSE` in your R script, or removing it.

If you have any problems with package installation or loading, please contact the facilitators for help before proceeding.


### File organization and housekeeping

In this tutorial, files are imported and outputs exported using relative file paths with the help of the `here` package. 

  1. For this to work seamlessly, we suggest that you download the folder containing all the course materials and save it on your desktop. 
  2. Next, open the folder and double-click on the the 'Mapping_UK.Rproj' file, which will open a new instance of RStudio.  
  3. If you click on the 'files' pane in RStudio, you should now be able to see all the sub-folders containing the course materials. 
  4. Go to the `File` menu in RStudio and select `New file --> R script`.
  5. Name the R script (e.g. `My_GIS_code.R`) and save it in the same folder.
  6. Add the `pacman::p_load()` function including the required packages as shown in the code chunk above to the top of your script, changing the `update` argument to `FALSE`.
  7. Run this code to load the required libraries into your current R session.

You are now ready to begin.

\newpage

------------------------------------------------------------------------

# Case study {.tabset .tabset-pills}


## Background

For this exercise, you have been provided with a line list of cases from an outbreak associated with raw drinking milk that was contaminated with shiga toxin-producing *Escherichia coli* (STEC), which occurred in 2014.

Initially, the outbreak was identified after 9 people who had visited a farm producing raw milk in the South West of England in September and October 2014 fell ill.  The farm routinely produced and sold raw milk at local markets, but was also open to visitors in the spring and summer time.  Visitors were exposed either through interacting with the animals and their environment or from drinking a sample of the raw milk.  Samples of raw milk from this farm were also tested and positive for the same strain of STEC as cases.

Sequencing the isolates from the 9 cases revealed that they fell into the same 5-single nucleotide polymorphism (SNP) cluster (i.e. were very closely related genetically).  Isolates from the raw drinking milk also fell into this cluster. 

Surprisingly, the whole genome sequencing cluster also included isolates from 36 other cases that were not known to have visited the implicated farm during the outbreak period.  Twelve of these cases were new (identified late 2014 or early 2015) and had not yet been associated with any outbreak.  The remaining additional cases were historic (identified between 2009 and 2012) and had been associated with other outbreaks linked to two farms and two schools which were also located in the South West of England.  The OCT decided to include the other cases associated with the historic outbreaks in the line list too.  

Information on the exposures of both historic and new cases was available, because the National STEC enhanced surveillance system had been implemented in 2009.  The routine enhanced surveillance questionnaire for STEC specifically asks about exposure to raw milk, farm visits and any overnight stays outside the normal place of residence in the 14 days before symptom onset. Where possible, the full addresses (including UK postcode) of any locations visited during the incubation period are recorded on the questionnaire.  

Routine whole genome sequencing for STEC was piloted from January 2014, with retrospective sequencing of randomly selected samples from all outbreaks investigated in the previous 5 years.   The outbreak control team is interested to know whether the other cases with isolates in the same 5-SNP WGS cluster may also have been exposed to raw milk or farm animals from the same region in England; to determine this they have asked you to undertake some geospatial analysis.

To conduct your analysis, the OCT has provided you with a case line list that includes information on whole genome sequencing clustering, association with investigated outbreaks, geospatial exposures, and exposure to raw animal products.  

The following variables will be critical for your analysis:

   + `wgs_rdm_cluster`: Indicates if case isolates are part of the WGS cluster associated with this raw drinking milk (RDM) outbreak (`TRUE` if yes, `FALSE` if they are from a different WGS cluster)
   + `epi_outbreak`: Indicates which outbreak investigation each case is linked to. Options are this outbreak (RDM 2014), Farm A, Farm B, School A, School B, or sporadic (not linked to an outbreak).
   + `postcode_home`: UK postcode of residence
   + `postcode_uk_travel`: UK postcode of other overnight stays in the 14 days before onset
   + `postcode_exposure`: UK postcode where exposure to raw milk most likely occurred.

Note that `postcode_uk_travel` only includes places other than the case's normal residence where they spent at least one night, while `postcode_exposure` is the postcode of the location where the OCT thinks exposure for that case most likely occurred; this also includes day visits to farms, or the case's own residence if they live close to the implicated farm.

Note that the `postcode_exposure` for all 9 cases in the `RDM 2014` is the postcode of the farm in the South West of England that had been identified as the source of infection for this outbreak.



## Data sets

For this tutorial, the following data sets have been provided:

  1. `CaseStudy_RDM_anon_data.csv` - raw data set
  2. `CaseStudy_RDM_anon_data_cleanpcodes.csv` - raw data with pre-cleaned postcodes
  3. `CaseStudy_RDM_anon_data_coords.csv` - raw data with geographic coordinates added
  4. `PHEC_population.csv` - regional population data for choropleth incidence maps

We suggest that you start working with the third data set, where the addresses have already been geocoded to GPS coordinates.  

More advanced users may wish to undertake the optional exercise in the appendix, which demonstrates how to clean postcodes with a function containing regular expressions and then geocodes them. 

The postcode cleaning function is a little complex, but it is explained in the appendix.

Note that geocoding requires an internet connection; it can sometimes be very slow, or not work if there is too much traffic to the server at the same time.


## Questions to answer

As explained in the introduction, the OCT were surprised to learn that some additional cases, both historic and new, were also linked to this outbreak via whole genome sequencing.  By mapping the cases in different ways and looking at their geo-spatial distribution, what conclusions can you draw about the following questions?

  1. Does the geo-spatial distribution of cases confirm or refute the hypothesis that the source of infection for all cases in the WGS cluster is in the South West region?
  2. Based on their geo-spatial distribution over time, are the historic cases likely to have been exposed to the same source of infection as cases from the current RDM outbreak or a different source?
  3. Overall, does the geo-spatial evidence best support the cluster defined by whole genome sequencing (`wgs_rdm_cluster`) or by epidemiological links (`epi_outbreak`)?


\newpage  

------------------------------------------------------------------------

# Geospatial investigations in R {.tabset .tabset-pills}

In this section, the example code below shows how to iteratively build different types of maps that will be helpful for your investigation.  To complete your exploration of the data and answer the questions, you will need to apply this code to the three different postcode locations, stratifying by WGS cluster, epi outbreak, or other variables that seem useful to you.


## Data prep


### Import data

In this section, we will read in (import) the raw data sets into R.  You can find the data sets in a sub-folder called `data` which should be in the folder of course materials provided to you for this exercise.

We will use the package `rio` to import the outbreak line list and regional population data, with the command `rio::import()`.  The `here` package is used to define the path to the files we want to import, as this package facilitates the use of relative file paths, which will work on any computer, provided you have the same participant pack folder.

After importing the data, we will use the `clean_names()` function from the `janitor` package.  This function removes non-alphanumeric characters and spaces from column names in a data set, to make them easier to reference in r code.  It will also change all the column names to lower case.  Spaces are replaced with an underscore.


```{r import csv files}

###########################################################################
# Import outbreak case linelist

# Import the outbreak case line list, which includes residence coordinates:
caselist <- rio::import(file = here("data", 
                                    "CaseStudy_RDM_anon_data_coords.csv")) %>%
  # Clean variable names:
  janitor::clean_names() %>% 
  
  # Create nice labels for the Raw drinking milk (RDM) cluster variable:
  dplyr::mutate(wgs_rdm_cluster = case_when(
    wgs_rdm_cluster == TRUE ~ "A. Raw milk cluster", 
    wgs_rdm_cluster == FALSE ~ "B. Other cluster")) 


###########################################################################
# Import population data for health regions

# Import the population data for health regions (called PHE centres):
region_pop <- rio::import(file = here("data", 
                                      "PHEC_population.csv")) %>% 
  
  # Clean variable names: 
  janitor::clean_names()

```


Next, we will import the shape files for the borders of 9 health regions (called PHE Centers or PHEC for short) that we will superimpose onto some of the maps. Shape files have a more complex structure, so we need to use the `sf` package to import them ('sf' stands for 'simple feature'). The `sf` package is part of the `tidyverse` so the shape files are imported as a `data.frame` which is easy to interrogate:

```{r import shape files}

# Import the shape files for 9 health regions (called PHEC or PHE Centres): 
region_sf <- sf::st_read(dsn = here("shapefiles", "En_PHE_Centre.shp")) %>% 
  
  # Clean variable names:
  janitor::clean_names()

```

You can see in the printed information about the imported shape files, that the coordinate reference system (CRS) used for the health regions is the British National Grid. It is important that the same coordinate reference system is used for the map tiles, for case addresses and for any borders superimposed on the map.  Case addresses have been mapped to Global Positioning System (GPS) coordinates, which is a different system, so we will have to transform the shape files to this system to make sure they match up.  

Fortunately, we can use the `sf::st_transform()` function to do this.  The main argument required is `crs` (coordinate reference system).  The CRS cannot be specified as a text string; instead we need to supply the EPSG code for the system we want to transform to, which is `4326`.  You can find more information about the EPSG codes of two common coordinate reference systems [here](https://geo.nls.uk/urbhist/guides_coordinates.html).

```{r}

region_sf <- sf::st_transform(x = region_sf, crs = 4326)

```


These shape files have been taken from the ESRI online shape file collection.  The collection has shape files for different administrative levels of most countries, but you normally need a licence for ArcGis software to access them.  However there are many free shapefile collections; for example the OpenDataSoft collection, which you can browse [here](https://public.opendatasoft.com/explore/dataset/world-administrative-boundaries/information/).


### Calculate crude incidence rates

In this section we will calculate crude incidence per 100 000 population by health region (the health regions are called PHE Centres in this data set) and add them to the health region shape file.  To do this, we will need to prepare the following:

  1. Create a summary table of case counts by PHE Centre from the data set `caselist`
  2. Add the population for each PHE Centre to this table from the data set `region_pop`
  3. Calculate incidence per 100 000 population as a new variable in the table
  3. Merge this incidence table with the health regions shapefile `region_sf`

Note that the names of the PHE Centres are stored in a variable called `phecnm` in all three of the data sets that you imported (case line list, region populations and region shape file).

```{r calculate incidence}

# Create the table of summary counts of cases by PHE Centre:
incidencetab <- caselist %>% 
  group_by(phecnm) %>% 
  summarise(cases = length(caseid))

# Add population data to the table:
incidencetab <- region_pop %>% 
  left_join(incidencetab, by = "phecnm")

# Change NA counts to 0 for incidence calculation:
incidencetab <- incidencetab %>% 
  mutate(cases = ifelse(is.na(cases), 0, cases))

# Calculate crude incidence per 100,000 population:
incidencetab <- incidencetab %>% 
  mutate(Incidence = round((cases/population)*100000, 2))

# Update the health regions shapefile by merging the incidence table with it:
region_sf <- region_sf %>% 
  left_join(incidencetab, by = "phecnm")

```

**Exploratory tasks:**

   + Inspect the `incidencetab` data.frame after each command to check what has changed

   + Try piping together all the commands to create the incidence table 
   + (hint: they are currently separated to make it easier to interogate each step)

   + Try rounding crude incidence to more or fewer decimal points
   + (hint: this is specified in `mutate(Incidence = round(..., 2))`).

Note that if some regions have very low incidence, you may wish to include more numbers after the decimal point - the aim is to easily distinguish between incidence rates in the different regions on a numeric scale, as we will see later when we create the choropleth map.


### Mapping cases

In the following sections, we will explore the spatial distribution of the cases by creating maps using the `sf`, `ggmap`, `ggplot2` and `leaflet` packages.  

For creating static maps, we will also need to download the coordinates that will define the edges of our background map.  We will do this using the `osmdata` package, which fetches background maps for a given boundary area from OpenStreetMap.

In the case linelist data (`caselist`), latitude and longitude are provided for three different types of address as follows :

  + `home_lat` and `home_long`: where the case lives (their normal residence)
  + `travel_lat` and `travel_long`: location of implicated day trips (e.g. farms visited) 
  + `exposure_lat` and `exposure_long`: where case was exposed (their residence or a farm)

These variables will be used for mapping the cases.


## Import base map:

To create a static map of cases, we will first import data to create a background map of the UK using the `osmdata` package.  You can define the boundary box for the map you want to import simply by typing the country name (or region and country name if you want to focus on one region).  A list of country names and their corresponding codes is provided on the [OpenStreetMap Nominatim country code wiki page](https://wiki.openstreetmap.org/wiki/Nominatim/Country_Codes).  Note that OSM is fairly flexible with country names and will usually accept commonly used abbreviations as well as the official name or code.


We will then use this boundary box to fetch a nice background map from OpenStreetMap with the `ggmap` package.  Note that it used to be possible to bulk geocode and define boundary boxes with Google maps as well, but this is now exclusively a paid service.  If your organisation does pay for this service, you can use it by providing an API key as one of the arguments.  For this exercise, we will only use OpenStreetMap as it is a free service.

This map will be used as the base layer.  


```{r boundary box UK}

# First, define the boundaries of the map you want to import:
ukbb <- osmdata::getbb(place_name = "United Kingdom", featuretype = "country")


# Next, get the base layer map matching this boundary box: 
ukmap <- ggmap::get_map(location = ukbb, 
                        maptype = "toner-background", 
                        source = "osm")

# Have a quick look at the base map:
ggmap(ukmap)


```


Getting a map of the whole of the UK produces a lot of empty space (ocean) because it includes some small islands to the north of Scotland.  In this outbreak, all the cases are actually located in England (a level 4 administrative boundary within the UK).  We can zoom in by creating a boundary box just for England instead (note that England is considered a 'settlement' within the 'country' of UK):

```{r boundary box England}

# First, define the boundaries of the map you want to import:
englandbb <- osmdata::getbb(place_name = "England", featuretype = "settlement")


# Next, get the base layer map matching this boundary box: 
englandmap <- ggmap::get_map(location = englandbb, 
                             maptype = "toner-background", 
                             source = "osm")

# Have a quick look at the base map:
ggmap(englandmap)

```


We already know that a lot of the cases are concentrated in the South West of England (near the farm that was the likely source of the contaminated raw milk).  We can be even more precise with the boundary box by supplying it with the range (maximum and minimum) of coordinates defining the area in which the cases are found:

```{r boundary box case coordinates}

# First define the boundary box with case coordinates:
cbbox <- ggmap::make_bbox(lon = home_long,
                          lat = home_lat, 
                          data = caselist,
                          f = 0.1)


# Next, get the base layer map matching this boundary box: 
coordinatesmap <- ggmap::get_map(location = cbbox, 
                                 maptype = "toner-background", 
                                 source = "osm")

# Have a quick look at the base map:
ggmap(coordinatesmap)

```

**Exploratory tasks:**

Note that when defining the boundary box, the `f` value (fraction by which the range should be extended beyond the minimum and maximum coordinates) will affect the zoom level of the map.  A higher resolution map will also include more place names at a lower administrative level.

   + Try changing the `f` value and see what looks best
   + Choose the base map that you think best defines the area of interest
   + (use this base map in the next section)



## Add case coordinates:

In this section we will add the case coordinates to the base map in a new layer with `ggmap` to create a point map.  You can also adjust the colour, shape and size of the points.


```{r point map basic}

# Create the case map:
pointmap <- ggmap::ggmap(coordinatesmap, 
                         extent = 'panel', 
                         maprange = FALSE) +
  geom_point(data = caselist, 
             aes(x = exposure_long, y = exposure_lat), 
             colour = "#238443",
             fill = "darkred",
             alpha = 0.5,
             size = 4, 
             shape = 21, 
             show.legend = TRUE) +
  labs(x = "Longitude", y = "Latitude")
  
# Have a look at the map:
pointmap  

# Save the map as a .pdf
ggplot2::ggsave(filename = "Map of cases - static.pdf", 
              plot = pointmap, 
              width = 9, 
              height = 9, 
              dpi = 300)

```
**Exploratory tasks:**

In the example code in this tutorial, we have only used one of the three addresses available for cases (their residence address, defined by `home_long` and `home_lat`).  The other two geocoded addresses available are for locations cases traveled to on a day trip (`travel_long` and `travel_lat`) and the location of likely exposure to the contaminated raw milk (`exposure_lat` and `exposure_long` - note this is a composit of the other two addresses).

   + Try recreating this case map, but use the exposure coordinates instead
   + (hint: change the x and y variables in the aes)
   + Repeat the case map again, but using the day trip (travel) coordinates instead
   + Which coordinates do you think are most informative?


## Stratify coordinates:

It might be useful to see how cases are distributed when they are stratified by a grouping variable.  Cases were originally defined in terms of time, place and STEC subtype.  However later, whole genome sequencing results became available.  This showed that not all case isolates fell into the the same whole genome sequencing cluster as the strain of STEC that had been found in the raw milk (hereafter referred to as the raw milk cluster). The variable that we will use for this stratification is called `wgs_rdm_cluster`.

```{r point map with stratification}

# Create the stratified map:
stratamap <- ggmap::ggmap(coordinatesmap, 
                         extent = 'panel', 
                         maprange = FALSE) +
  geom_point(data = caselist, 
             aes(x = home_long, y = home_lat, fill = wgs_rdm_cluster), 
             colour = "black",
             alpha = 0.5,
             size = 4, 
             shape = "circle filled", 
             show.legend = TRUE) +
  labs(x = "Longitude", y = "Latitude") + 
  scale_fill_manual(values = c("darkred", "turquoise")) + 
  labs(fill = "Whole genome sequencing cluster") + 
  scale_size(guide = "none")

  
# Have a look at the map:
stratamap  

# Save the map as a .pdf
ggplot2::ggsave(filename = "Map of cases - stratified by clade.pdf", 
              plot = stratamap, 
              width = 9, 
              height = 9, 
              dpi = 300)

```
**Exploratory tasks:**

In the above code, we specified how the points should look, with the `colour` (of shape border), `size`, `shape`, and `alpha` (transparency level of fill colour) arguments.  We also specified exactly which colours to use for the stratified variable with `scale_fill_manual()`. You can find more information about the different formatting options for points and other ggplot features by typing `vignette("ggplot2-specs")` in your R console (the vignette will open in the help pane of RStudio).

   + Try changing the `alpha` (transparency) - what effect does it have on overlapping points?
   + Try changing the `shape` to `circle` - what happens to the border colour of the points?
   + Try changing the variables to stratify on - what else could be interesting?
   + (hint: change the fill variable in aes)
   + (hint: make sure you provide enough colours for each factor level in `scale_fill_manual`)
   + (hint: change the title of the legend in `labs(fill = ...)`)


## Shape files

In this section, we will use some shape files of PHE Centres (9 health regions in England) to create the base layer for our map and plot the points on top, as before.  Remember that we already imported the shape files and saved them as an object called `region_sf` at the beginning.

This time, we will use the `ggplot2` package to first create the empty frame for the map, as it is not possible to have an empty frame with the `ggmap` package.

Once the shape files have added as a base layer, we can add the stratified case coordinates on top using the same code as for the previous map.

```{r static map with shapefile stratified by WGS clade}

# Plot the shapefile as the base layer and add case coordinates on top:
phecmap <- ggplot2::ggplot() + 
  geom_sf(data = region_sf, 
          colour = "black", 
          fill = "darkgreen",
          alpha = 0.5,
          size = 0.75) +
  geom_point(data = caselist, 
             aes(x = home_long, y = home_lat, fill = wgs_rdm_cluster), 
             colour = "black",
             alpha = 0.5,
             size = 4, 
             shape = "circle filled", 
             show.legend = TRUE) +
  labs(x = "Longitude", y = "Latitude") + 
  scale_fill_manual(values = c("darkred", "turquoise")) + 
  labs(fill = "Whole genome sequencing cluster")

# View the map:
phecmap

# Save the map:
ggplot2::ggsave(filename = "Map of cases - stratified with shapefile.pdf", 
                plot = phecmap, 
                width = 9, 
                height = 9, 
                dpi = 300)


```
**Exploratory tasks:**

In this map, we first started with the shapefile layer, where we specified how we would like the base map to look (colour and thickness of region borders, fill colour and transparency).

   + Try changing the base map fill colour and transparency (alpha)
   + (hint: change the parameters in `geom_sf`)
   + Select the background map and points colour and transparency levels that work best.


## Overlapping points

One challenge with static point maps is that if the points overlap, it is difficult to see what the density of cases is in a given area.  In this section, we will explore four different ways of overcoming this problem:

  1. Producing separate maps for each time interval 
  2. Creating contour and heat maps of case counts
  3. Creating a choropleth map of crude incidence rates per head of population
  4. Creating an interactive map with zoom-regulated clustering

We will not be looking at ways to analyse spatial density as this falls outside the scope of this practical, but the impact of selected area size on relative density should be taken into account when interpreting these types of maps.  


## Time series maps:

In addition to stratifying by the fill colour of the point, maps can also easily be stratified by separating them out according to a grouping variable such as time, with the function `ggplot2::facet_wrap()`.  

In this example, we will look at the previous map and stratify it by year of onset to investigate the change in case distribution over time.

```{r static map time series}

# Facet phecmap by year:
tsmap <- phecmap + facet_wrap(~year, ncol = 3)

# View the maps:
tsmap

# Save the maps:
ggplot2::ggsave(filename = "Map of cases - stratified by year.pdf", 
                plot = tsmap, 
                width = 9, 
                height = 9, 
                dpi = 300)

```



## Heatmaps

To create this map we will begin as we did in the previous section, using the shapefile as our base layer and adding the coordinates on top. Two basic options are available to add a heatmap layer:

  1. `geom_density2d()` which adds contour lines
  2. `stat_density2d()` which adds shades of colour indicating increasing density
  
You can increase the number of `bins` and `alpha` arguments to view case distributions at a higher resolution or finer grain, respectively.

```{r contourmap of case counts}

# Create the contour map with geom_density2d:
contourmap <- ggplot2::ggplot() + 
  geom_sf(data = region_sf, 
          colour = "black", 
          fill = "#004529",
          alpha = 0.5,
          size = 0.75) +
  coord_sf() + 
  geom_density2d(data = caselist, 
                 mapping = aes(x = home_long, 
                               y = home_lat, 
                               alpha = 0.5),
                 inherit.aes = FALSE, 
                 contour_var = "count")

# View the contour map:
contourmap

# Save the contour map:
ggplot2::ggsave(filename = "Map of cases - heatmap with contours.pdf", 
                plot = contourmap, 
                width = 9, 
                height = 9, 
                dpi = 300)
```


```{r heatmap of case counts}

# Create the heatmap with stat_density2d:
heatmap <- ggplot2::ggplot() + 
  geom_sf(data = region_sf, 
          colour = "black", 
          fill = "#004529",
          alpha = 0.5,
          size = 0.75) +
  coord_sf() + 
  stat_density2d(data = caselist, 
                 mapping = aes(x = home_long, 
                               y = home_lat, 
                               fill = ..level.., 
                               alpha = ..level..), 
                 size = 0.01,  
                 bins = 50, # Changing the bins will change how the map looks
                 geom = "polygon", 
                 inherit.aes = FALSE) + 
  scale_fill_gradient(low = "blue", 
                      high = "red", 
                      guide = "none") + 
  scale_alpha(range = c(0, 0.5),
              guide = "none")

# View the heatmap with colour levels:
heatmap

# Save the heatmap:
ggplot2::ggsave(filename = "Map of cases - heatmap with shading.pdf", 
                plot = heatmap, 
                width = 9, 
                height = 9, 
                dpi = 300)

```


## Choropleth maps


Now that we have added incidence rate to the `sf` object, we can create the choropleth map:

```{r choropleth map of incidence}

# Create the choropleth map:
cimap <- ggplot2::ggplot(phecll) + 
  
  # Set the geom as Incidence rate from the sf object:
  geom_sf(mapping = aes(fill = Incidence), 
          colour = "black", 
          size = 0.5) +
  
  # Plot latitutde and longitude from the sf object:
  coord_sf() + 
  
  # Specify how the fill colours should be set:
  scale_fill_distiller(
    # Use shades of blue for the fill
    palette = "Blues", 
    # Break up the incidence scale into 8 groups:
    breaks = scales::breaks_pretty(n = 8), 
    # Use the default order of colours (-1 to reverse):
    direction = 1) +
  
  # Set the legend title and colour order in the legend:
  guides(fill = guide_legend(title = "Crude incidence per 100 000", 
                             reverse = FALSE)) + 
  
  # Apply the ggplot theme_nothing to remove grid lines:
  theme_nothing(legend = TRUE)

# View the map:
cimap
  
# Save the map:
ggplot2::ggsave(filename = "Map of cases - choropleth incidence.pdf", 
                plot = cimap, 
                width = 9, 
                height = 9, 
                dpi = 300)

```

Note there are a few things you may wish to change or explore, specifically:

   + Try reversing the order of the colours
   + Change the number of breaks from 8 to 4 and then 16
   
The number of breaks you use will determine how easy or difficult it is to view differences in incidence between regions on the map.  A small number of breaks may be sufficient when the differences are large, but in this data set it is more challenging because there is one region with much higher incidence than all the others.  The appropriate number of breaks to use will depend on what you are trying to show in the map; is it sufficient to highlight the region implicated as the source of the outbreak, or do you also want to show which other regions were affected?


## Interactive maps

One final option we can explore for separating out overlapping data points is the creation of an interactive map.  To create the map, we will use the `leaflet` package, with the clustering feature `clusterOptions` activated.  This feature "collapses" cases when the map is zoomed out and represents the cases in a circular marker with a numeric total.  When zoomed in, individual cases will segregate.

Leaflet maps can be included in html reports and are a very useful way to display multiple aspects of the data set in one figure, as they can be richly annotated.  We will use the `popup` command to annotate each point with some data about the case from our case dataset.  Note that because the leaflet map is a html object, we need to create the annotations using html syntax for line breaks (`<br/>`). 

If desired, you can also superimpose the boundaries of health regions ontop of a leaflet map, with the `addPolygons()` command, using an sf (shape file) object as input.  An example of this is shown in the code below.

When distributing leaflet maps for public consumption, it is advisable set a maximum zoom limit, to control how much viewers can zoom in.  In the code below, the zoom maximum is set to the default level (18), which allows viewers to zoom right in to street level and see the exact building where a case plotted on the map lives.  Depending on who you are sharing the map with, this may be too much personal identifying information.  Try changing `maxZoom` to lower numbers (e.g. 12) to see the effect.  

On the other hand, keeping the max zoom at a high level can be very useful for investigating outbreaks that have micro-spatial patterns, such as street-level variations or where proximity to a contaminated source of infection is important (e.g. waterworks or cooling towers).



```{r interactive leaflet map}

# Create the interactive map:
rdmleaflet <- leaflet() %>% 
  # Add open street map (default base layer)
  addTiles(options = tileOptions(maxZoom = 18)) %>% 
  # Add transformed shapefile of regions
  addPolygons(data = phecll, weight = 5, col = "black") %>% 
 # Add markers for case residence with descriptive labels:
  addMarkers(lng = rdmdata$home_long,
             lat = rdmdata$home_lat,
             popup = paste("ID: ", rdmdata$caseid, "<br/>",
                           "Epilink: ", rdmdata$epi_outbreak, "<br/>", 
                           "RDM clade: ", rdmdata$wgs_rdm_cluster), 
             clusterOptions = markerClusterOptions()) 

# View the map:
rdmleaflet
  
# Save the map:
htmlwidgets::saveWidget(widget = rdmleaflet, 
                        file = "Map of cases - interactive.html")


```


# References

Further information about this outbreak and other spatial analyses performed is provided in the below publication:

Butcher H, Elson R, Chattaway MA, Featherstone CA, Willis C, Jorgensen F, Dallman T, Jenkins C, McLauchlin J, Beck C, & Harrison S (2016). Whole genome sequencing improved case ascertainment in an outbreak of Shiga toxin-producing Escherichia coli O157 associated with raw drinking milk. Epidemiology and Infection, 144(13), 2812-2823. [https://doi.org/10.1017/S0950268816000509](https://doi.org/10.1017/S0950268816000509)


# Appendix

Following feedback on this exercise from outbreak module participants in December 2021, additional details have been provided below on two more complex techniques that are included in the code.


## A. Cleaning address data with regular expressions

### Postcode cleaning:

Because the `tidygeocoder` package is not UK-specific, it will not recognise UK postcodes unless they are in the correct format and the country name is also supplied.

A quick preview of the postcode data with `head(rdmdata$postcode_home)` for example, shows you that the space that normally separates the in-code and out-code part of UK postcodes is missing for postcodes with 7 characters.  

We can correct this by using a regular expression to add the spaces back in.  The function below first checks if there is already a space in the postcode; if there is, it returns it as-is.  If the space is missing, it counts back 3 characters from the end of the postcode (right side) and adds a space before the third-last character.  Note that this function is not vectorised, so we need to loop over individual rows (postcodes) to apply it to the data.

```{r clean_postcodes, message=FALSE, warning=FALSE}

###############################################################################

# Function to ensure space between incode and outcode in postcodes:
add_space <- function(postcodevar){
  
  # If a space is present between incode and outcode, return as is:
  if(grepl(pattern = "?(\\s)", x = postcodevar) == TRUE){
    
    pcwithspace = postcodevar
  
    } else {
    # If a space is missing between incode and outcode, add one:
    pcwithspace = stringr::str_replace(string = postcodevar, 
                               pattern = "(?=.{3}$)",
                               replacement = " ")
    }
  # Return the postcodes with space added:
  return(pcwithspace)
}

###############################################################################

# Read in the raw data set (where postcodes are not in the correct format):
rdmdata <- rio::import(file = here("data", "CaseStudy_RDM_anon_data.csv"))

# Use the add_space function to correct postcode format:
rdmdata <- rdmdata %>% 
  rowwise() %>% 
  mutate(postcode_home = add_space(postcode_home)) %>% 
  mutate(postcode_uk_travel = add_space(postcode_uk_travel)) %>% 
  mutate(postcode_exposure = add_space(postcode_exposure))


```



### More on regular expressions

In the above code, postcodes were re-formatted to include a space between the outcode and incode with a function called `add_space`.  This function used regular expressions, or "regex" to determine if a space was already present in the code or not, and where missing, add the space as the fourth-last character in the text string.  

While this has nothing specifically to do with mapping, it is quite useful to understand and be able to construct regular expressions as they are language agnostic (the same expressions can be used in STATA, python, SQL etc.) and can be used for many data cleaning tasks (wherever patterns within text strings need to be identified, extracted or replaced). 

Although this topic is too extensive to cover in this appendix, the following resources are a useful starting point:

  1. [Regular expressions every R programmer should know](https://www.jumpingrivers.com/blog/regular-expressions-every-r-programmer-should-know/) - this blog post introduces users to some basics.
  2. [A complete beginner's guide to regular expressions in R](https://regenerativetoday.com/a-complete-beginners-guide-to-regular-expressions-in-r/) - this is a tutorial on how to construct regular expressions, with some worked examples
  3. [R help page on regular expressions](https://stat.ethz.ch/R-manual/R-devel/library/base/html/regex.html) - although most regular expressions are language agnostic, a few shortcuts for certain commonly used expressions (for example searching for a string that contains only letters) have been adapted specifically for R.  This R help page details these shortcuts and a few other essentials. 


## B. Geocoding

## Geocoding the data

**Geocoding** (the process of converting addresses to geographic coordinates) is done by using an address-to-coordinates lookup table, which is typically accessed from either a public database or a local mirror.  It can be challenging to geocode using the public databases as in order to ensure fair use, these often have a cap on the number of postcodes you can geocode at one time.  Conversely, geocoding from a local mirror database created by your institution requires permission to access that database from your organisation and can generally only be accessed via your organisational account.  

In this tutorial, we will use the package `tidygeocoder`, which can derive geographic coordinates (longitude and latitude) given a full address from any country.  As UK postcodes are sufficiently specific to provide a longitude and latitude for a given address, we can geocode these with `tidygeocoder` provided that we also provide the country name.  In preparation for this step, we will add a column to the dataset called `country`: 



```{r add_country, message=FALSE, warning=FALSE}

# Add a new column to the case data indicating the country:
rdmdata <- rdmdata %>% 
  mutate(country = "UK")

```


## Geocoding cleaned postcodes:

We can now geocode the cleaned postcodes.

For this practical exercise, we will be using the publicly available package [tidygeocoder](https://jessecambon.github.io/tidygeocoder/) from CRAN, which is tidyverse compatible and performs batch look-ups of addresses and postcodes (geocoding) or geographic coordinates (reverse geocoding).  Matching values are returned as new columns, which you can add directly to your data.frame.  

The package interfaces with [multiple data providers](https://jessecambon.github.io/tidygeocoder/articles/geocoder_services.html), each of which have their own terms and conditions for performing geocoding batch queries.  We will use the OpenStreetMap (OSM) service Nominatum, which is a free service that will work with addresses or postcodes from any country.  Results are returned at a rate of 1 query per second.

Note that OSM may block users who perform the same queries multiple times (see their terms of use [here](https://operations.osmfoundation.org/policies/nominatim/)).  If you encounter any difficulties during this practical session, please skip this step and use the pre-geocoded dataset `CaseStudy_RDM_anon_data_coords.csv` instead.


```{r geocode, warning=FALSE}
# Use the residential (home) postcodes and country to fetch geocoordinates:
rdmdata <- rdmdata %>% 
  tidygeocoder::geocode(postalcode = postcode_home, 
                        country = country, 
                        method = "osm", 
                        lat = "home_lat", 
                        long = "home_long") %>% 
  tidygeocoder::geocode(postalcode = postcode_uk_travel, 
                        country = country, 
                        method = "osm", 
                        lat = "travel_lat", 
                        long = "travel_long") %>% 
  tidygeocoder::geocode(postalcode = postcode_exposure, 
                        country = country, 
                        method = "osm", 
                        lat = "exposure_lat", 
                        long = "exposure_long")

```



## C. Calculating density in contour and heatmaps

The function that actually calculates density is buried within other functions called by `ggplot2`, which ultimately determine the colour intensities on a heatmap, or the distance between contour lines for a contour map.  Below is a short primer explaining the basics.  Some suggested reading is also included at the end.

Contour maps and heat maps are ways of visualising the density of a population (or cases).  **Population density is the number of people per unit area**.  To visualise this on a map, we need to divide the map into squares representing our unit area.  For example, if we want to know the density of the population per square meter, we would divide up the map into squares, each one representing one square meter.  If there are two points (cases) in one square, this square has a density of 2, and so on.  If we changed the unit area to 0.5 meters squared, fewer points might fall within that area, so in this example our density might be reduced from 2 to 1 single case.  Equally, if we increase the unit area to e.g. 10 meters squared, one "square" might now include a lot more cases, e.g. 50. Changing the unit area (or other factors that influence density) in a contour or heatmap has a similar effect to changing the bins in a choropleth map.  

Choosing an appropriate unit for the area to calculate density is important as it will change how the map looks.  In the code we used in this practical, the argument `alpha` within `stat_density2d` to produce the heatmap is assigned to "level".  This is a bit obscure, but "level" is the name of a new variable that R creates in the data set while preparing the plot, and R fills this new variable with a density estimate.  

The default density estimate value is calculated by `ggplot2` in the background, using the `kde2d` function (kernel density estimation in 2 dimensions) from the `MASS` package.  This function in turn uses a "normal reference distribution", also known as "Silverman's rule of thumb" to calculate the bandwidths.  Selecting the appropriate method to calculate bandwidths for density estimates is a complex topic.  

Some resources on bandwidth calculation have been provided below:

  1. [The importance of kernel density estimation bandwdith](https://aakinshin.net/posts/kde-bw/) - this very readable blog post explains the history behind different methods for calculating bandwidths and sign-posts the relevant functions in R, Python, Matlab and Wolfram Mathematica.
  2. [Contours of a 2D density estimate](https://ggplot2.tidyverse.org/reference/geom_density_2d.html) - this vignette shows how density estimates are calculated when making graphics with `ggplot2`.